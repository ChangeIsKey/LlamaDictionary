# Automatically Generated Definitions and their utility for Modeling Word Meaning
This is the official repository for our paper _Automatically Generated Definitions and their utility for Modeling Word Meaning_

## Table of Contents
- [Abstract](#abstract)
- [Getting Started](#getting-started)
- [Reproducing Results](#reproducing-results)
  -  [Download Data](#download-data)
  -  [Data Augmentation](#data-augmentation)
  -  [Data Evaluation](#data-evaluation)
  -  [Fine-tuning](#fine-tuning)
  -  [Model Evaluation](#model-evaluation)
  -  [WiC-WSD-LSC Evaluation](#WiC-WSD-LSC-evaluation)
- [References](#references)

## Abstract
Regardless of unprecedented advancements in text generation, modeling lexical semantics is still a challenging tasks often suffering from interpretability pitfalls. In this paper, we delve into the generation of dictionary-like sense definitions 
by using fine-tuned Llama models and explore their utility for modeling word meaning. Firstly, we evaluate the quality of the generated definitions on existing benchmarks, setting new state-of-the-art results for the Definition Generation task. 
Next, we explore the use of definitions generated by our models as intermediate representations subsequently encoded as sentence embeddings. We evaluate this approach on lexical semantics tasks such as the Word-in-Context, Word Sense Induction, Lexical Semantic Change, setting new state-of-the-art results in all three tasks. 

## Getting Started
Our research leveraged The <a href="https://www.c3se.chalmers.se/about/Alvis/">Alvis</a> cluster, a national <a href="https://www.naiss.se/">NAISS</a> (National Artificial Intelligence and Supercomputing System) resource specifically designed for Artificial Intelligence and Machine Learning investigations. To facilitate result reproducibility, we offer <a href="https://slurm.schedmd.com/sbatch.html">sbatch</a> files that streamline the process. If you intend to use NAISS resources, simply edit these files to include your NAISS project ID and run them with `sbatch`. Alternatively, you can directly run them with `bash` in other computing environments. 

Before you begin, ensure you have met the following requirements:
- <img src="https://miro.medium.com/v2/resize:fit:1400/1*lSTuwS4exV_s__kcShxk8w.png" width="20" height="20"> Python 3.11.3
- <img src="https://cdn-images-1.medium.com/max/580/0*Kt5_0uGLlCFAgbt6.png" width="25" height="25"> Required Python packages (listed in `requirements.txt`)

If you are using a cluster, you can direcyly load Python 3.11 with:

```module load PyTorch/2.1.2-foss-2023a-CUDA-12.1.1```

To install the required packages, you can create a virtual environment and use pip:

```bash
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

## Reproducing Results


### Download data
Download data used in <a href="https://aclanthology.org/2023.acl-long.176/">(Giulianelli et al., ACL 2023)</a> . 
```bash 
sbatch sbatch/download.sh
```

Convert data in <a href="https://jsonlines.org/">jsonl</a> format and process it for prompting LLMs.
```bash 
sbatch sbatch/format_data.sh
```

### Data Augmentation
Before augmenting data for the entire dataset, we checked the correctness of data augmentation for a small subset. Specifically, we randomly sampled 100 definitions, produced 10 usages for each of those definitions, and asked human annotators to check their correctness.

##### Data Evaluation
Before proceeding with data augmentation using ChatGPT-3.5, we decided to test a small set of generated data. For this purpose, we randomly sampled a small dataset for validation, performed data augmentation with ChatGPT-3.5, and then validated it through human assessment. To execute these steps, run:

```bash 
bash sbatch/validate_data_to_check.sh
```
Please note, we recommend using `bash` instead of `sbatch` here since the computation is very fast and requires low resources. Additionally, we welcome your feedback if you prefer to perform the data validation on your own. For the sake of reproducibility, we have commented out the command to re-execute the validation. If you are interested in doing this, please check inside `sbatch/data_to_check.sh`.

##### Data Augmentation
We then performed a random sampling of a more substantial number of definitions and used ChatGPT-3.5 to augment the data. To work with our data, run this command:
```bash
python sample_data_to_check.py -o data/data_to_augment.jsonl -n 15000
unzip word_usage_examples_generated_by_gpt-3.5-turbo.zip
mv word_usage_examples_generated_by_gpt-3.5-turbo.jsonl data/augmented_data.jsonl
```
Instead, if you are interested in repeating the data augmentation step, please check inside `sbatch/data_to_check.sh` and run this command:
```bash
sbatch sbatch/data_augmentation.sh
```
### Data partitioning
We split the data into a training set, a development set (Dev), and a test set. Both the development and test sets are further divided into two subsets: one containing examples with TARGETS (i.e., words or definitions) present in the training vocabulary (IN vocabulary), and the other containing examples with targets not present in the training vocabulary (OUT of vocabulary). The IN data sets consist of examples for target words/glosses that were seen in the training data, although with different examples. The OUT data sets contain target words/glosses not seen in the training data.

The split does not precisely match the specified sizes for training, development, and test sets because the splits are calculated based on targets/glosses. To obtain our partitions, execute the following script:

```bash
sbatch sbatch/split_data.sh
```
This create partition both at the word-level and word definition-level.

### Fine-tuning
Reproducing all our fine-tuning at once is possible using our comprehensive script, `sbatch/finetuning.sh`. However, please be aware that this may consume a significant amount of time and could be interrupted if your system lacks sufficient GPU resources. Therefore, we highly recommend reviewing the script and executing each command line by line to maintain full control over the experiments.

You find our sbatch files in the `sbatch` folder and our Python code in the `src` folder. Feel free to contact us if you face any issues!

To reproduce all our fine-tuning at once, run:
```bash 
sbatch sbatch/finetuning.sh
```

tmp
```bash 
#!/bin/bash
#SBATCH -A NAISS2024-5-148 -p alvis
#SBATCH -t 6-23:00:00
#SBATCH --gpus-per-node=A100:4

export CUDA_VISIBLE_DEVICES="1,2,3,4"
export HF_DATASETS_CACHE="/mimer/NOBACKUP/groups/cik_data/fra_hf_cache"
export HF_HOME="/mimer/NOBACKUP/groups/cik_data/fra_hf_cache"

python sft_trainer.py -m meta-llama/Meta-Llama-3-8B -o llama3-tuned -b 128 --gradient_accumulation_steps 2 --lora_rank 128
python sft_trainer.py -m meta-llama/Llama-2-7b-chat-hf -o llama2-chat-tuned -b 128 --gradient_accumulation_steps 2 --lora_rank 128
python sft_trainer.py -m meta-llama/Llama-2-7b-chat-hf -o llama2-chat-tuned-32 -b 128 --gradient_accumulation_steps 2 --lora_rank 32
python sft_trainer.py -m meta-llama/Meta-Llama-3-8B -o llama3-tuned-32 -b 128 --gradient_accumulation_steps 2 --lora_rank 32
```



#### Model Evaluation
#### WiC-WSD-LSC Evaluation
